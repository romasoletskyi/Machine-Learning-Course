{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward neural network. Backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observing processes in the human brain, scientists created a mathematical model in 40s, which mimic the behavior of neurons' systems. It became so successful that now it is being used in biology and computer science because it is very good in pattern recognition, something that usual computers cannot do.\n",
    "Let's start our journey in a world of neural networks from a neuron, the simplest 'cell' that every network has."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let a neuron have n inputs and 1 output (axon). Each input has its **magnitude** $x_i$ and each connection has its **weight** $w_i$. Then output of the neuron is $$y=\\sigma\\Bigg(\\sum_{i=1}^n x_iw_i \\Bigg)$$ Letter $\\sigma$ denotes **activation** function (we will discuss them more at the end of this notebook). Inputs are usually coming from other neurons or from initial input. Outputs are ususally coming into other neurons or into final output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neurons are usually grouped into **layers**. There are different types of connections between layers, which define a type of neural network. The simplest type is **feedforward** neural network, whose layers are fully connected or, in other words, each neuron of a previous layer is connected with every neuron of a next layer. \n",
    "There are 3 different types of layers in feedforward neural network: input layer (all neurons have one input from an external source), hidden layer, and output layer (all neurons' output values exit network). In result, a neural network has one input layer, n hidden layers ($n\\geqslant 0$), and one output layer. This structure can be treated as a black box with its input connected to an input layer and its output to an output layer, as shown in the picture.\n",
    "![Wikipedia](https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/296px-Colored_neural_network.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If treat the neural network as a function with several inputs and outputs, let's denote input values as a list of numerical values **x** and output values as a list **y**. Then, basic algorithm of calculating **y** list is:\n",
    "1. Assign values to outputs of input layer neurons as **x** values\n",
    "2. If a neural network has hidden layer, compute outputs of hidden layer neurons. If there is more than one hidden layer, repeat step 2.\n",
    "3. Compute **y**, which is equal to outputs of output layer neurons\n",
    "\n",
    "This algorithm will be implemented in the second part of this lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of NN in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we are already familiar with objects like numbers and lists in Python, sometimes we need to create objects ourselves with properties we desire. Such task is possible using a special structure in Python called class. Usually class has the following structure:\n",
    "```python\n",
    "class Name_of_class:\n",
    "    def __init__(self,initial_parameters):\n",
    "        self.parameters=initial_parameters\n",
    "        \n",
    "    def function1(self,input_values):\n",
    "        do something\n",
    "        return output_values\n",
    "```\n",
    "The most important elements are self, which refers to class object, and init function, which initializes new object and object's *attributes*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you still don't understand how this works, don't worry! Now we are going to create the class called \"Triangle\", which creates object \"Triangle\" by setting coordinates of its vertexes (which are, basically, initial parameters). Then this \"Triangle\" can tell the length of its sides, its area and plot itself: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Triangle:\n",
    "    def __init__(self,x1,y1,x2,y2,x3,y3):\n",
    "        #writing coordinates\n",
    "        self.x_coordinates=[x1,x2,x3]\n",
    "        self.y_coordinates=[y1,y2,y3]        \n",
    "        \n",
    "    def length_of_sides(self):\n",
    "        #calculating sides' lengths\n",
    "        a=((self.x_coordinates[0]-self.x_coordinates[1])**2+(self.y_coordinates[0]-self.y_coordinates[1])**2)**(1/2)\n",
    "        b=((self.x_coordinates[1]-self.x_coordinates[2])**2+(self.y_coordinates[1]-self.y_coordinates[2])**2)**(1/2)\n",
    "        c=((self.x_coordinates[2]-self.x_coordinates[0])**2+(self.y_coordinates[2]-self.y_coordinates[0])**2)**(1/2)\n",
    "        return a,b,c\n",
    "    \n",
    "    def area(self):\n",
    "        #getting sides' length\n",
    "        a,b,c=self.length_of_sides()\n",
    "        #calculating area from Heron's formula\n",
    "        p=(a+b+c)/2\n",
    "        s=(p*(p-a)*(p-b)*(p-c))**(1/2)\n",
    "        return s\n",
    "    \n",
    "    def plot(self):\n",
    "        import matplotlib.pyplot as plt\n",
    "        x_data=[self.x_coordinates[0],self.x_coordinates[1],self.x_coordinates[2],self.x_coordinates[0]]\n",
    "        y_data=[self.y_coordinates[0],self.y_coordinates[1],self.y_coordinates[2],self.y_coordinates[0]]\n",
    "        plt.plot(x_data,y_data)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After declaring class, we can create objects and use their properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1=Triangle(0,0,1,2,3,0)\n",
    "print(t1.length_of_sides())\n",
    "print(t1.area())\n",
    "t1.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also access all self. values like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t1.x_coordinates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class \"NeuralNet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering that neural network is a separate entity, which is described by many factors (weights, architecture, and activation function) and has different purposes (output of y-labels, training, which will be discussed in second part of this lecture), it is quite reasonable to create a new class called \"NeuralNet\". This class will represent feedforward neural network with one input, one hidden and one output layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weights represent 'brain' of the neural net because by changing them we can completely change net's behavior. We can consider an implementation of weight in Python by looking at the picture below.\n",
    "Firstly, each neuron in the input layer has the number of connection equal to the number of neurons in the hidden layer. This means that the total number of weights in input-hidden connection is equal to a product of the number of the neurons in both layers. As a result, the best way of representing these weights is a double list or list of lists in Python and table (or matrix) in mathematics.  Let the first index of a weight be an index of an input layer neuron and the second one be an index of a hidden layer neuron. For example, the first matrix of the neural net from the picture below can be written as:\n",
    "\\begin{bmatrix}\n",
    "    w_{11}       & w_{12} & w_{13} & w_{14} \\\\\n",
    "    w_{21}       & w_{22} & w_{23} & w_{24} \\\\    \n",
    "    w_{31}       & w_{32} & w_{33} & w_{34} \\\\  \n",
    "\\end{bmatrix}\n",
    "By analogy, let the first index of a weight be an index of a hidden layer neuron and the second one be an index of an output layer neuron in the second weight table:\n",
    "\\begin{bmatrix}\n",
    "    w_{11}       & w_{12}\\\\\n",
    "    w_{21}       & w_{22}\\\\    \n",
    "    w_{31}       & w_{32}\\\\  \n",
    "    w_{41}       & w_{42}\\\\  \n",
    "\\end{bmatrix}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Wikipedia](https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/296px-Colored_neural_network.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is common to initialize weights as random variables from an interval $(-\\epsilon,+\\epsilon)$ where $\\epsilon$ is quite small number (around 0.01-0.1). Using that knowledge, we can implement weights in Python as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "weights=[]\n",
    "n_input=3\n",
    "n_hidden=4\n",
    "epsilon=0.1\n",
    "for i in range(n_input):\n",
    "    weights_row=[] #creating empty row of weights from one input neuron\n",
    "    for j in range(n_hidden):\n",
    "        weights_row.append(random.uniform(-epsilon,epsilon)) #appending random weights\n",
    "    weights.append(weights_row) #appending row to a table of weights    \n",
    "\n",
    "print(weights) #printing weights\n",
    "for i in range(n_input): #printing weights in more table-like form\n",
    "    print(weights[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of different activation functions for different purposes, but they all have in common one simple thing - they can't be linear. Let's list 3 the most common ones and their derivatives and then plot their graphs\n",
    "1. Sigmoid function $$f(x)=\\frac{1}{1+e^{-x}}$$ $$f'(x)=f(x)(1-f(x))$$\n",
    "2. TanH (hyperbolic tangent) $$f(x)=\\frac{e^x-e^{-x}}{e^x+e^{-x}}$$ $$f'(x)=1-f^2(x)$$\n",
    "3. Rectified linear unit (ReLU) \n",
    "$$\n",
    "f(x)=\n",
    "\\begin{cases} \n",
    "      0 & x<0 \\\\\n",
    "      x & x\\geqslant0\n",
    "\\end{cases}\n",
    "$$\n",
    "$$\n",
    "f'(x)=\n",
    "\\begin{cases} \n",
    "      0 & x<0 \\\\\n",
    "      1 & x\\geqslant0\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x_data=np.arange(-5,5,0.01)\n",
    "y1_data=1/(1+np.exp(-x_data))\n",
    "y2_data=(np.exp(x_data)-np.exp(-x_data))/(np.exp(x_data)+np.exp(-x_data))\n",
    "y3_data=x_data*(x_data>0)\n",
    "\n",
    "fig=plt.figure(figsize=(12,4))\n",
    "ax1=fig.add_subplot(131)\n",
    "ax1.set_title('Sigmoid')\n",
    "ax1.plot(x_data,y1_data)\n",
    "\n",
    "ax2=fig.add_subplot(132)\n",
    "ax2.set_title('TanH')\n",
    "ax2.plot(x_data,y2_data)\n",
    "\n",
    "ax3=fig.add_subplot(133)\n",
    "ax3.set_title('ReLU')\n",
    "ax3.plot(x_data,y3_data)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use sigmoid function which can be implemented using only basic library math:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+math.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sigmoid(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural net's shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to implement neural net but without calculating output and training weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    def __init__(self,inp,hid,out,epsilon):\n",
    "        #initial parameters are numbers of neurons in input (inp), hidden (hid), and output (out) layers \n",
    "        #plus epsilon value for weight initialization\n",
    "        #firstly, we write down numbers of neurons in each layer \n",
    "        self.inp=inp\n",
    "        self.hid=hid\n",
    "        self.out=out\n",
    "        #secondly, we create weights\n",
    "        #wa - weights of connection between input and hidden layers\n",
    "        #wb - weights of connection between hidden and output layers\n",
    "        import random\n",
    "        self.wa=[]\n",
    "        self.wb=[]\n",
    "        \n",
    "        for i in range(self.inp):\n",
    "            weights_row=[] \n",
    "            for j in range(self.hid):\n",
    "                weights_row.append(random.uniform(-epsilon,epsilon)) \n",
    "            self.wa.append(weights_row)  \n",
    "            \n",
    "        for i in range(self.hid):\n",
    "            weights_row=[] \n",
    "            for j in range(self.out):\n",
    "                weights_row.append(random.uniform(-epsilon,epsilon)) \n",
    "            self.wb.append(weights_row) \n",
    "            \n",
    "    #define activation function\n",
    "    def sigmoid(self,x):\n",
    "        return 1/(1+math.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn=NeuralNet(3,4,2,0.1) #implement neural net from a picture\n",
    "print(nn.wa)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
